---
layout: post
title: Word2Vecä»¥åŠtfå®ç°
date: 2018-03-10
tags: AI 
---

### å¼•è¨€ï¼Ÿ

å¤§éƒ¨åˆ†å†…å®¹æ‘˜è‡ªã€ŠTensorFlowå®æˆ˜ã€‹ï¼ˆé»„æ–‡åšï¼‰ï¼Œç®—æ˜¯è¯»ä¹¦ç¬”è®°ï¼Œä¹Ÿä¼šåœ¨ç½‘ä¸Šæœé›†ä¸€äº›ç›¸å…³çš„æ–‡ç« è¿›è¡Œè¡¥å……ã€‚
æˆ‘è§‰å¾—è¿™æœ¬ä¹¦å¾ˆniceå‘€ï¼Œè™½ç„¶æ²¡æœ‰å¯¹å…·ä½“çš„ç¥ç»ç½‘ç»œè¿›è¡Œè¯¦ç»†çš„è®²è§£ï¼Œä½†æ˜¯å¾ˆé€šä¿—æ˜“æ‡‚ï¼Œåå®æˆ˜ã€‚
ä¹Ÿä¼šæœ‰çœ‹äº†ç¥ç»ç½‘ç»œçš„è¯¦è§£ç„¶åé›¨é‡Œé›¾é‡Œï¼Œå†åœ¨è¿™è¾¹çœ‹äº†æ€»ç»“çš„è±ç„¶å¼€æœ—ä¹‹æ„Ÿã€‚

### One-Hot Encoder

æœ€æ—©çš„è¯å‘é‡è¡¨ç¤ºæ–¹æ³•ä¸ºOne-Hot Encoderï¼Œå°†å­—è¯è½¬æˆç¦»æ•£çš„å•ç‹¬çš„ç¬¦å·ï¼š

    æ­å· [0,0,0,0,0,0,0,1,0,â€¦â€¦ï¼Œ0,0,0,0,0,0,0]
    ä¸Šæµ· [0,0,0,0,1,0,0,0,0,â€¦â€¦ï¼Œ0,0,0,0,0,0,0]
    å®æ³¢ [0,0,0,1,0,0,0,0,0,â€¦â€¦ï¼Œ0,0,0,0,0,0,0]
    åŒ—äº¬ [0,0,0,0,0,0,0,0,0,â€¦â€¦ï¼Œ1,0,0,0,0,0,0]

æ­å·ã€ä¸Šæµ·ã€å®æ³¢ã€åŒ—äº¬å„å¯¹åº”ä¸€ä¸ªå‘é‡ï¼Œå‘é‡ä¸­åªæœ‰ä¸€ä¸ªå€¼ä¸º1ï¼Œå…¶ä½™éƒ½ä¸º0ã€‚
ä½¿ç”¨One-Hot Encoderæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå³å¯¹ç‰¹å¾çš„ç¼–ç æ˜¯éšæœºçš„ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°å­—è¯ä¹‹é—´çš„å…³ç³»ã€‚å¦‚åŒ—äº¬ã€ä¸Šæµ·åº”è¯¥èšé›†åˆ°ä¸€èµ·ï¼Œåç››é¡¿ã€çº½çº¦èšé›†åœ¨ä¸€èµ·ã€‚å¹¶ä¸”æ•ˆç‡ä½ã€è®¡ç®—éº»çƒ¦ã€‚

### ä»€ä¹ˆæ˜¯Word2Vec

å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯åœ¨NLPé¢†åŸŸæœ€å¸¸ä½¿ç”¨çš„ç¥ç»ç½‘ç»œï¼Œè€ŒWord2Vecåˆ™æ˜¯å°†è¯­è¨€ä¸­çš„å­—è¯è½¬åŒ–ä¸ºè®¡ç®—æœºå¯ä»¥ç†è§£çš„ç¨ å¯†å‘é‡ï¼ˆDense Vectorï¼‰ï¼Œé€šè¿‡ä¸€ä¸ªåµŒå…¥ç©ºé—´ä½¿å¾—è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å•è¯åœ¨è¯¥ç©ºé—´å†…è·ç¦»å¾ˆè¿‘ã€‚

Word2Vecåˆ†ä¸ºï¼š

    CBOWï¼ˆContinuous Bag of Wordsï¼‰:ä»åŸå§‹è¯­å¥ï¼ˆä¾‹å¦‚ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯____ï¼‰æ¨æµ‹ç›®æ ‡å­—è¯ï¼ˆä¾‹å¦‚ï¼šåŒ—äº¬ï¼‰

    Skip-Gram:ä»ç›®æ ‡å­—è¯æ¨æµ‹å‡ºåŸå§‹è¯­å¥ï¼Œåœ¨å¤§å‹è¯­æ–™åº“ä¸­è¡¨ç°æ›´å¥½

`è´Ÿé‡‡æ ·`ï¼šåœ¨Word2Vecçš„CBOWæ¨¡å‹ä¸­ï¼Œåªè®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»æ¨¡å‹ï¼ŒåŒºåˆ†çœŸå®çš„ç›®æ ‡è¯æ±‡å’Œç¼–é€ çš„å™ªå£°è¯æ±‡ï¼ˆNegative Samplingï¼‰ã€‚åªéœ€è¦è®¡ç®—éšæœºé€‰æ‹©çš„kä¸ªè¯æ±‡è€Œå¹¶éå…¨éƒ¨ã€‚åœ¨å®é™…ä¸­ï¼Œä½¿ç”¨Noise-Contrastive Estimationï¼ˆNCEï¼‰Lossï¼Œåœ¨tfä¸­å¯¹åº”ä¸º`tf.nn.nce_loss()`

### æ¨¡å‹æ‹†è§£

Word2Vecæ¨¡å‹å…¶å®å°±æ˜¯ç®€å•åŒ–çš„ç¥ç»ç½‘ç»œï¼š

<img src="/images/posts/2018/03/Word2Vec//1.jpg" height="500" width="800"> 

å›¾ç‰‡æ¥æº:[ç†è§£ Word2Vec ä¹‹ Skip-Gram æ¨¡å‹](https://zhuanlan.zhihu.com/p/27234078),ä½œè€…å†™çš„è¶…çº§è¯¦ç»†ï¼Œå¯ä»¥å»è¯»è¯»ã€‚

è¾“å…¥æ˜¯One-Hot Vectorã€‚
å¦‚æœæƒ³è¦ç”¨300ä¸ªç‰¹å¾æ¥è¡¨ç¤ºä¸€ä¸ªå•è¯ï¼Œåˆ™éšè—å±‚çš„ç»´åº¦ä¸º300ï¼Œä¸”Hidden Layeræ²¡æœ‰æ¿€æ´»å‡½æ•°ã€‚
Output Layerç»´åº¦è·ŸInput Layerçš„ç»´åº¦ä¸€æ ·ï¼Œç”¨çš„æ˜¯Softmaxå›å½’ï¼Œè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒã€‚

å°†ä»è¾“å…¥å±‚åˆ°éšå«å±‚çš„é‚£äº›æƒé‡ï¼Œä½œä¸ºæ¯ä¸€ä¸ªè¯æ±‡è¡¨ä¸­çš„è¯çš„å‘é‡ã€‚

å› ä¸ºOne-Hot Vectorååˆ†ç¨€ç–ï¼Œæ¶ˆè€—å¤§é‡çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è¿›è¡Œé«˜æ•ˆè®¡ç®—ï¼Œåªè¦é€‰æ‹©é€‰æ‹©çŸ©é˜µä¸­å¯¹åº”çš„å‘é‡ä¸­ç»´åº¦å€¼ä¸º1çš„ç´¢å¼•è¡Œï¼š

<img src="/images/posts/2018/03/Word2Vec//4.jpg" height="150" width="500"> 

å·¦è¾¹å‘é‡ä¸­å–å€¼ä¸º1çš„å¯¹åº”ç»´åº¦ä¸º3ï¼ˆä¸‹æ ‡ä»0å¼€å§‹ï¼‰ï¼Œé‚£ä¹ˆè®¡ç®—ç»“æœå°±æ˜¯çŸ©é˜µçš„ç¬¬3è¡Œï¼ˆä¸‹æ ‡ä»0å¼€å§‹ï¼‰â€”â€” [10, 12, 19]ï¼Œè¿™æ ·æ¨¡å‹ä¸­çš„éšå±‚æƒé‡çŸ©é˜µä¾¿æˆäº†ä¸€ä¸ªâ€æŸ¥æ‰¾è¡¨â€œï¼ˆlookup tableï¼‰

è¯¦ç»†æ‹†è§£å‚è€ƒ[å¦‚æœçœ‹äº†æ­¤æ–‡è¿˜ä¸æ‡‚ Word2Vecï¼Œé‚£æ˜¯æˆ‘å¤ªç¬¨](http://www.sohu.com/a/128794834_211120)

1.  CBOW    

å°†ä¸€ä¸ªè¯æ‰€åœ¨çš„ä¸Šä¸‹æ–‡ä¸­çš„è¯ä½œä¸ºè¾“å…¥ï¼Œè€Œé‚£ä¸ªè¯æœ¬èº«ä½œä¸ºè¾“å‡ºï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œçœ‹åˆ°ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå¸Œæœ›å¤§æ¦‚èƒ½çŒœå‡ºè¿™ä¸ªè¯å’Œå®ƒçš„æ„æ€ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå¤§çš„è¯­æ–™åº“è®­ç»ƒï¼Œå¾—åˆ°ä¸€ä¸ªä»è¾“å…¥å±‚åˆ°éšå«å±‚çš„æƒé‡æ¨¡å‹ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¬¬lä¸ªè¯çš„ä¸Šä¸‹æ–‡è¯æ˜¯iï¼Œjï¼Œkï¼Œé‚£ä¹ˆiï¼Œjï¼Œkä½œä¸ºè¾“å…¥ï¼Œå®ƒä»¬æ‰€åœ¨çš„è¯æ±‡è¡¨ä¸­çš„ä½ç½®çš„å€¼ç½®ä¸º1ã€‚ç„¶åï¼Œè¾“å‡ºæ˜¯lï¼ŒæŠŠå®ƒæ‰€åœ¨çš„è¯æ±‡è¡¨ä¸­çš„ä½ç½®çš„å€¼ç½®ä¸º1ã€‚è®­ç»ƒå®Œæˆåï¼Œå°±å¾—åˆ°äº†æ¯ä¸ªè¯åˆ°éšå«å±‚çš„æ¯ä¸ªç»´åº¦çš„æƒé‡ï¼Œå°±æ˜¯æ¯ä¸ªè¯çš„å‘é‡ã€‚ä¾‹å¦‚ç¬¬iä¸ªè¯çš„è¯å‘é‡ä¸º`(Wi,1 Wi,2...Wi,m)`,mä¸ºå‘é‡çš„ç»´åº¦ã€‚

<img src="/images/posts/2018/03/Word2Vec//2.jpeg" height="500" width="700">

2.  Skip-gram

å°†ä¸€ä¸ªè¯æ‰€åœ¨çš„ä¸Šä¸‹æ–‡ä¸­çš„è¯ä½œä¸ºè¾“å‡ºï¼Œè€Œé‚£ä¸ªè¯æœ¬èº«ä½œä¸ºè¾“å…¥ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å‡ºä¸€ä¸ªè¯ï¼Œå¸Œæœ›é¢„æµ‹å¯èƒ½å‡ºç°çš„ä¸Šä¸‹æ–‡çš„è¯ã€‚é€šè¿‡åœ¨ä¸€ä¸ªå¤§çš„è¯­æ–™åº“è®­ç»ƒï¼Œå¾—åˆ°ä¸€ä¸ªä»è¾“å…¥å±‚åˆ°éšå«å±‚çš„æƒé‡æ¨¡å‹ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¬¬lä¸ªè¯çš„ä¸Šä¸‹æ–‡è¯æ˜¯iï¼Œjï¼Œkï¼Œé‚£ä¹ˆiï¼Œjï¼Œkä½œä¸ºè¾“å‡ºï¼Œå®ƒä»¬æ‰€åœ¨çš„è¯æ±‡è¡¨ä¸­çš„ä½ç½®çš„å€¼ç½®ä¸º1ã€‚ç„¶åï¼Œè¾“å…¥æ˜¯lï¼ŒæŠŠå®ƒæ‰€åœ¨çš„è¯æ±‡è¡¨ä¸­çš„ä½ç½®çš„å€¼ç½®ä¸º1ã€‚è®­ç»ƒå®Œæˆåï¼Œå°±å¾—åˆ°äº†æ¯ä¸ªè¯åˆ°éšå«å±‚çš„æ¯ä¸ªç»´åº¦çš„æƒé‡ï¼Œå°±æ˜¯æ¯ä¸ªè¯çš„å‘é‡ã€‚

<img src="/images/posts/2018/03/Word2Vec//3.jpeg" height="500" width="700">

### tfå®ç°Word2Vec

ä½¿ç”¨Skip-Gramæ¨¡å¼çš„Word2Vecï¼Œä»¥â€œthe quick brown fox jumped over the lazy dogâ€ä¸ºä¾‹ï¼Œè®­ç»ƒæ ·æœ¬ä¸º(quick,the)ï¼Œ(quick,brown),(brown,quick),(brown,fox)ç­‰ã€‚è®­ç»ƒæ—¶ï¼Œå¸Œæœ›æ¨¡å‹å¯ä»¥ä»ç›®æ ‡è¯æ±‡quickæ¨æµ‹å‡ºè¯­å¢ƒtheï¼ŒåŒæ—¶ä¹Ÿéœ€è¦åˆ¶é€ éšæœºçš„è¯æ±‡ä½œä¸ºè´Ÿæ ·æœ¬ï¼ˆå™ªå£°ï¼‰ã€‚ä½¿ç”¨SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼‰æ¥æ›´æ–°æ¨¡å‹ä¸­Word Embeddingçš„å‚æ•°ï¼Œè®©æ¦‚ç‡åˆ†å¸ƒçš„æŸå¤±å‡½æ•°(NCE Loss)å°½å¯èƒ½å°ã€‚è¿™æ ·æ¯ä¸ªå•è¯çš„Embedded Vectorå°±ä¼šéšç€è®­ç»ƒè¿‡ç¨‹ä¸æ–­è°ƒæ•´ï¼Œç›´åˆ°å¤„äºä¸€ä¸ªæœ€åˆé€‚è¯­æ–™çš„ç©ºé—´ä½ç½®ã€‚

åŸºæœ¬æ¯ä¸€å¥éƒ½æœ‰æ³¨é‡Šå•¦ï¼Œå¯ä»¥ç»“åˆåŸä¹¦æ¥çœ‹æ­¤æ®µä»£ç ğŸ˜³

å•Šå•Šå•Šï¼Œæˆ‘æ‰‹å·¥å·§äº†ä¸€éä»£ç ï¼Œæ‰å‘ç°ä»£ç å±…ç„¶æ˜¯tfä¸Šçš„[demo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)...

{% highlight python %}

# -*- coding: UTF-8 -*-
import collections
import math
import os
import random
import zipfile
import numpy as np 
import urllib
import tensorflow as tf 
import urllib

# ---------------------------  æ•°æ®é¢„å¤„ç†---------------------------
url = 'http://mattmahoney.net/dc/'
#   ä»â€˜http://mattmahoney.net/dcâ€™ä¸‹è½½æ–‡æœ¬æ–‡ä»¶ï¼Œé‡Œé¢çº¦æœ‰17005207ä¸ªç”¨ç©ºæ ¼åˆ†éš”å¥½çš„è‹±æ–‡å¥å­ã€‚
def maybe_download(filename,expected_bytes):
    if not os.path.exists(filename):
        filename, _ = urllib.request.urlretrieve(url + filename,filename)
    statinfo = os.stat(filename)

    if statinfo.st_size == expected_bytes:
        print('Found and verified',filename)
    else:
        print(statinfo.st_size)
        raise Exception(
            'Failed to verify' + filename +'. Can you get to it with a browser?'
        )
    return filename

filename = maybe_download("text8.zip",31344016)
#   filename = "text8.zip"

#   è§£å‹æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨`tf.compat.as_str`å°†æ•°æ®è½¬åŒ–æˆå•è¯åˆ—è¡¨ã€‚
def read_data(filename):
    with zipfile.ZipFile(filename) as f:
       data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

words = read_data(filename)
print('Data size',len(words))

#   åˆ›å»ºvocabularyè¯æ±‡è¡¨,é€‰å–å‰50000é¢‘æ•°çš„å•è¯ï¼Œå…¶ä½™å•è¯è®¤å®šä¸ºUnknownï¼Œç¼–å·ä¸º0
vocabulary_size = 50000
def build_dataset(words):
    count = [['UNK',-1]]
    print("---------------------------------")
    #   collections.Counterç»Ÿè®¡å•è¯åˆ—è¡¨ä¸­å•è¯çš„é¢‘æ•°
    #   most_common å–top 50000é¢‘æ•°çš„å•è¯ä½œä¸ºvocabulary
    #   count = [(å•è¯1,è¯é¢‘1),(å•è¯2,è¯é¢‘2),...]
    count.extend(collections.Counter(words).most_common(vocabulary_size -1))
    dictionary = dict()
    #   å­˜å…¥dicä¸­
    for word ,_ in count:
        dictionary[word] = len(dictionary)  

    #   éå†å•è¯åˆ—è¡¨ï¼Œå¦‚æœå‡ºç°åœ¨dictionaryä¸­ï¼Œåˆ™è½¬åŒ–ä¸ºç¼–å·ã€‚ä¸åœ¨åˆ™ä¸º0ã€‚
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    #   å­—å…¸çš„åè½¬å½¢å¼ï¼Œå¯ç”¨ç¼–å·æŸ¥è¯¢å‡ºå¯¹åº”çš„å•è¯
    reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))
    return data,count,dictionary,reverse_dictionary
data,count,dictionary,reverse_dictionary = build_dataset(words)
#   åˆ é™¤åŸå§‹å•è¯åˆ—è¡¨ï¼ŒèŠ‚çº¦å†…å­˜
del words
#   æ‰“å°æœ€é«˜é¢‘å‡ºç°çš„è¯æ±‡åŠæ•°é‡ï¼Œ[['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
print('Most common words (+UNK)',count[:5])
#   æ‰“å°dataå‰10ä¸ªå•è¯
print('Sample data',data[:10],[reverse_dictionary[i] for i in data[:10]])

#   ---------------------------ç”ŸæˆWord2Vecçš„è®­ç»ƒæ ·æœ¬---------------------------
#   ä½¿ç”¨Skip-Gramæ¨¡å¼ï¼Œå°†åŸå§‹æ•°æ®ï¼š
#   â€œthe quick brown fox jumped over the lazy dogâ€è½¬åŒ–ä¸ºï¼š
#   (quick,the)ï¼Œ(quick,brown),(brown,quick),(brown,fox)ç­‰ã€‚
data_index = 0
def generate_batch(batch_size , num_skips, skip_window): #  ç”Ÿæˆè®­ç»ƒç”¨çš„batchæ•°æ®
    #   skip_window å•è¯æœ€è¿œå¯ä»¥è”ç³»çš„è·ç¦»
    #   num_skips å¯¹æ¯ä¸ªå•è¯ç”Ÿæˆå¤šå°‘æ ·æœ¬
    global data_index   #   å®šä¹‰ä¸ºå…¨å±€å˜é‡ï¼Œå› ä¸ºä¼šåå¤è°ƒç”¨æ­¤å‡½æ•°
    #   batch_size å¿…é¡»ä¸ºnum_skipsçš„æ•´æ•°å€ï¼Œç¡®ä¿æ¯ä¸ªbatchåŒ…å«ä¸€ä¸ªè¯æ±‡å¯¹åº”çš„æ‰€æœ‰æ ·æœ¬
    assert batch_size % num_skips == 0  
    assert num_skips <= 2 * skip_window
    #   ndarrayå¯¹è±¡æ˜¯ç”¨äºå­˜æ”¾åŒç±»å‹å…ƒç´ çš„å¤šç»´æ•°ç»„
    batch = np.ndarray(shape = (batch_size),dtype = np.int32)
    labels = np.ndarray(shape = (batch_size,1),dtype = np.int32)
    #   æŸä¸ªå•è¯åˆ›å»ºæ ·æœ¬æ—¶ä¼šä½¿ç”¨åˆ°çš„å•è¯æ•°é‡,åŒ…æ‹¬å•è¯æœ¬èº«å’Œå®ƒå‰åçš„å•è¯
    span = 2 * skip_window + 1
    #   åŒå‘é˜Ÿåˆ—ï¼Œä½¿ç”¨appendæ–¹æ³•æ·»åŠ å˜é‡æ—¶ï¼Œåªä¼šä¿ç•™æœ€åæ’å…¥spanä¸ªå˜é‡
    buffer = collections.deque(maxlen=span) 
    #   ä»åºå·data_indexå¼€å§‹ï¼ŒæŠŠspanä¸ªå•è¯é¡ºåºè¯»å…¥bufferä½œä¸ºåˆå§‹å€¼
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1 ) % len(data)

    for i in range(batch_size // num_skips):    # // ä¸ºæ•´æ•°é™¤æ³•
        target = skip_window    #   bufferä¸­ç¬¬skip_windowä¸ªå˜é‡ä¸ºç›®æ ‡å•è¯
        targets_to_avoid = [ skip_window ]  #   ç”Ÿæˆæ ·æœ¬æ—¶éœ€è¦é¿å…çš„å•è¯åˆ—è¡¨
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0,span-1)
            targets_to_avoid.append(target)
            batch[i*num_skips +j] = buffer[skip_window]
            labels[i*num_skips +j ,0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index +1) % len(data)
    return batch ,labels
#   ç®€å•æµ‹è¯•åŠŸèƒ½
batch, labels = generate_batch(batch_size = 8, num_skips = 2,skip_window = 1)
for i in range(8):
    print(batch[i],reverse_dictionary[batch[i]],'->',labels[i,0],
        reverse_dictionary[labels[i,0]])

#   è®­ç»ƒæ•°æ®
batch_size = 128
embedding_size = 128    # å•è¯è½¬ä¸ºç¨ å¯†å‘é‡çš„ç»´åº¦ï¼Œä¸€èˆ¬æ˜¯50~1000
skip_window = 1     # å•è¯é—´æœ€è¿œå¯ä»¥è”ç³»çš„è·ç¦» 
num_skips = 2   # æ¯ä¸ªç›®æ ‡å•è¯æå–çš„æ ·æœ¬æ•°

#   éªŒè¯æ•°æ®ï¼ŒéšæœºæŠ½å–ä¸€äº›é¢‘æ•°æœ€é«˜çš„å•è¯ï¼Œçœ‹å‘é‡ç©ºé—´ä¸Šè·Ÿå®ƒä»¬æœ€è¿‘çš„å•è¯æ˜¯å¦ç›¸å…³æ€§æ¯”è¾ƒé«˜
valid_size = 16     # æŠ½å–çš„éªŒè¯å•è¯æ•°
valid_window = 100  # éªŒè¯å•è¯åªä»é¢‘æ•°æœ€é«˜çš„100ä¸ªå•è¯ä¸­æŠ½å–
valid_examples = np.random.choice(valid_window,valid_size,replace=False)
num_sampled = 64    # è®­ç»ƒæ—¶ç”¨æ¥åšè´Ÿæ ·æœ¬çš„å™ªå£°å•è¯çš„æ•°é‡

#   ---------------------------å®šä¹‰Skip-Gramæ¨¡å‹çš„ç½‘ç»œç»“æ„---------------------------
graph = tf.Graph()
with graph.as_default():
    train_inputs = tf.placeholder(tf.int32,shape = [batch_size])
    train_labels = tf.placeholder(tf.int32,shape = [batch_size,1])
    #   å°†éšæœºäº§ç”Ÿçš„valid_examplesè½¬æ¢ä¸ºconstant
    valid_dataset = tf.constant(valid_examples,dtype = tf.int32)

    with tf.device('/cpu:0'):
        #   tf.random_uniforméšæœºç”Ÿæˆæ‰€æœ‰å•è¯çš„è¯å‘é‡embeddings,å•è¯è¡¨å¤§å°ä¸º50000ï¼Œå‘é‡ç»´åº¦ä¸º128
        embeddings = tf.Variable(
            tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
        #   tf.nn.embedding_lookup æŸ¥æ‰¾è¾“å…¥train_inputså¯¹åº”çš„å‘é‡
        embed = tf.nn.embedding_lookup(embeddings,train_inputs)
        #   ä½¿ç”¨NCE Lossä½œä¸ºè®­ç»ƒçš„ä¼˜åŒ–ç›®æ ‡
        nce_weights = tf.Variable(
            tf.truncated_normal([vocabulary_size,embedding_size],
                                stddev=1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
    #   lossçš„è®¡ç®—æ–¹å¼
    loss = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weights,
                                         biases = nce_biases,
                                         labels = train_labels,
                                         inputs = embed,
                                         num_sampled = num_sampled,
                                         num_classes = vocabulary_size))

    #   å®šä¹‰ä¼˜åŒ–å™¨ä¸ºSGDï¼Œlrä¸º1.0
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
    #   è®¡ç®—åµŒå…¥å‘é‡embeddingsçš„L2èŒƒæ•°norm
    norm  = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims = True))
    #   è§„èŒƒåŒ–
    normalized_embeddings = embeddings /norm
    #   tf.nn.embedding_lookup æŸ¥è¯¢éªŒè¯å•è¯çš„åµŒå…¥å‘é‡
    valid_embeddings = tf.nn.embedding_lookup(
        normalized_embeddings, valid_dataset)
    #   è®¡ç®—éªŒè¯å•è¯çš„åµŒå…¥å‘é‡ä¸è¯æ±‡è¡¨ä¸­æ‰€æœ‰å•è¯çš„ç›¸ä¼¼æ€§
    similarity = tf.matmul(valid_embeddings,normalized_embeddings,transpose_b = True)
    #   åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å‚æ•°
    init = tf.global_variables_initializer()

#   æœ€å¤§çš„è¿­ä»£æ¬¡æ•°
num_steps = 100001
with tf.Session(graph = graph) as session:
    init.run()
    print("Initialized")
    average_loss = 0
    for step in range(num_steps):
        #   ç”Ÿæˆä¸€ä¸ªbatchçš„inputså’Œlabelsæ•°æ®
        batch_inputs,batch_labels = generate_batch(
            batch_size,num_skips,skip_window)
        feed_dict = {train_inputs:batch_inputs,train_labels:batch_labels}
        _,loss_val = session.run([optimizer,loss],feed_dict = feed_dict)
        average_loss +=loss_val
        # 2000æ­¤å¾ªç¯ï¼Œè®¡ç®—ä¸€ä¸‹å¹³å‡losså¹¶æ˜¾ç¤ºå‡ºæ¥
        if step % 2000 ==0:
            if step >0 :
                average_loss /= 2000
            print("Average loss at step ", step, ": ",average_loss)
            average_loss = 0

        # æ¯10000æ¬¡å¾ªç¯ï¼Œè®¡ç®—ä¸€æ¬¡éªŒè¯å•è¯ä¸å…¨éƒ¨å•è¯çš„ç›¸ä¼¼åº¦ï¼Œå¹¶å°†æœ€ç›¸ä¼¼çš„8ä¸ªå•è¯å±•ç¤ºå‡ºæ¥
        if step % 10000 ==0:
            sim = similarity.eval()
            for i in range(valid_size):
                valid_word = reverse_dictionary[valid_examples[i]]
                top_k = 8
                nearest = (-sim[i,:]).argsort()[1:top_k+1]
                log_str = "Nearest to %s: " % valid_word
            for k in range(top_k):
                close_word = reverse_dictionary[nearest[k]]
                #close_word = reverse_dictionary.get(nearest[k])
                log_str = "%s %s ," % (log_str,close_word)
            print(log_str)
    final_embeddings = normalized_embeddings.eval()

#   ---------------------------å¯è§†åŒ–---------------------------
import matplotlib.pyplot as plt
# low_dim_embs æ˜¯é™ç»´åˆ°2ç»´çš„å•è¯çš„ç©ºé—´å‘é‡ï¼Œåœ¨å›¾è¡¨ä¸­å±•ç¤ºæ¯ä¸ªå•è¯çš„ä½ç½®
def plot_with_labels(low_dim_embs,labels,filename = 'tsne.png'):
    assert low_dim_embs.shape[0] >= len(labels),"More labels than embeddings"
    plt.figure(figsize = (18,18))
    for i,label in enumerate(labels):
        x, y = low_dim_embs[i,:]
        plt.scatter(x,y)    # æ˜¾ç¤ºæ•£ç‚¹å›¾ï¼ˆå•è¯çš„ä½ç½®ï¼‰
        # plt.annotateä¸ºå•è¯æœ¬èº«
        plt.annotate(label,     
                     xy = (x,y),
                     xytext = (5,2),
                     textcoords = 'offset points',
                     ha = 'right',
                     va = 'bottom')
    plt.savefig(filename)   #   ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°

# é™ç»´,å°†åŸå§‹çš„128ç»´åµŒå…¥å‘é‡é™åˆ°2ç»´ï¼Œå±•ç¤ºè¯é¢‘æœ€é«˜çš„100ä¸ªå•è¯
from sklearn.manifold import TSNE
tsne = TSNE(perplexity=30, n_components=2,init="pca",n_iter = 5000)
plot_only = 100
low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])
labels = [reverse_dictionary[i] for i in range(plot_only)]
plot_with_labels(low_dim_embs,labels)

{% endhighlight %}



