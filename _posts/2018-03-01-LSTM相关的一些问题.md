---
layout: post
title: LSTM相关的一些问题
date: 2018-03-01
tags: AI
---

### 前言

最近在看深度学习相关的东西，就把一些自己想了很久，网上的答案也不是很详细的东西记录下来，可能会有部分错误，会不断改进哒。


### RNN 与 LSTM 相关基础

网上看到的RNN的网络图如下：

<img src="/images/posts/2018/03/LSTM//RNN-shorttermdepdencies.png" height="280" width="600"> 

但是也可以将RNN的网络图画成如下的模式：

<img src="/images/posts/2018/03/LSTM//LSTM3-SimpleRNN.png" height="280" width="600"> 

是不是很眼熟！对，LSTM只是在RNN的基础上进行了扩展，添加了很多gate来控制之前的记忆，以解决RNN可能引起的梯度弥散与梯度爆炸(这里可以参考[YJango的循环神经网络——实现LSTM](https://zhuanlan.zhihu.com/p/25518711))的问题。

传统的LSTM的网络图看上去特别复杂:

<img src="/images/posts/2018/03/LSTM//LSTM3-chain.png" height="280" width="600"> 

但是只要清楚一点，就是里面那么多的gate只是为了控制每次输入多少、遗忘多少、输出多少信息。通过学习和训练，来调节这个gate的值，以达到最好的效果。类似于一个阀门控制水流的多少，具体阀门的松紧即gate的值，需要通过反复学习训练来得到。具体可以参考：

[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[YJango的循环神经网络——实现LSTM](https://zhuanlan.zhihu.com/p/25518711)

因此，很多LSTM想不清楚的问题，可以直接用RNN来类比啦🤗~

### LSTM的输入与输出

首先还是看下RNN的输入与输出吧(摘自[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))：

<img src="/images/posts/2018/03/LSTM/charseq.jpeg" height="400" width="500">  

    An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). 
    This diagram shows the activations in the forward pass when the RNN is fed the characters "hell" as input. 
    The output layer contains confidences the RNN assigns for the next character (vocabulary is "h,e,l,o"); 
    We want the green numbers to be high and red numbers to be low.

将"hello"这个单词的每次字母进行One-Hot编码，每输入一个字母预测下一个输出的字母是什么。例如，输入h对应输出e，输入e对应输出l，而输入l则可能对应l或者o。此处对应4维的输入与输出层，一个有着三个神经元的隐藏层。

| batch_size  |  input_size  |  time_steps  |  num_units  |  layer_num  |  class_num  | 
| :----:      |:----:        |:----:        |:----:       |:----:       |:----:       |
| 1           |  4           |   4          |  3          |   1         |  4          |

再看下这个知乎回答上的这个图,可能会更加清晰一些:
<img src="/images/posts/2018/03/LSTM/知乎RNN图.jpg" height="400" width="600">  

[作者：Scofield](https://www.zhihu.com/question/41949741/answer/318771336)

所以，LSTM的输入与输出是怎样的呢？可以完全类比RNN的输入与输出。

<img src="/images/posts/2018/03/LSTM/myLSTM.jpg" height="600" width="850">  

这张图我手工画的，可能有不对的地方，主要参考了[Understanding LSTM in Tensorflow(MNIST dataset)](https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/)
 
 困惑了我最久的是num_units到底是什么，然后[知乎](https://www.zhihu.com/question/41949741/answer/309529532)这个回答好详细！

<img src="/images/posts/2018/03/LSTM/singleLSTMcell.png" height="300" width="500">  

图中四个黄色的框框就是传统的前馈神经网络，而num_units就是这个前馈神经网络里面的隐藏节点啊啊啊啊。

### 关于张量(tensor)的一些疑惑

神经网络的处理单位均为`向量`，所以`张量`是什么？

主要参考这篇文章[数学不行还学AI-第4话-图解张量](https://zhuanlan.zhihu.com/p/25268020),写的通俗易懂，我就不摘抄啦噜。

<img src="/images/posts/2018/03/LSTM/tensor.jpg" height="500" width="700">  

常见的张量有：

    3D = 时间序列   (time, frequency, channel)

    4D = 图像      (sample_size, width, height, color_depth)

    5D = 视频      (sample_size, frames, width, height, color_depth))

总而言之，张量想要多少维就多少维，就看你的需求与设计啦。

### 暂时就写这么多吧，想到了再补充



